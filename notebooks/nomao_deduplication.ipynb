{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53557d22",
   "metadata": {},
   "source": [
    "Introduction\n",
    "\n",
    "This notebook analyzes the Nomao dataset to solve a deduplication problem using machine learning. The goal is to predict whether two records refer to the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b8cadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.6)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2830ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b11fbf",
   "metadata": {},
   "source": [
    "Data Loading and Exploration\n",
    "\n",
    "We begin by loading the dataset and exploring the data types, missing values, and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b451427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/1dqw3mgx5g3g2kwgxm4w2l5r0000gn/T/ipykernel_22821/3792633630.py:2: DtypeWarning: Columns (57,58,59,60,61,62,65,66,67,68,69,70,101,102,103,105,106,107,117) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/Nomao/Nomao.data\")\n"
     ]
    }
   ],
   "source": [
    "# Load Data \n",
    "df = pd.read_csv(\"../data/Nomao/Nomao.data\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea7cc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Shape: (34464, 120)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34464 entries, 0 to 34463\n",
      "Columns: 120 entries, 0#1 to +1\n",
      "dtypes: float64(6), int64(1), object(113)\n",
      "memory usage: 31.6+ MB\n",
      "None\n",
      "                  1           1.1           1.2           1.3           1.4  \\\n",
      "count  34464.000000  34464.000000  34464.000000  34464.000000  34464.000000   \n",
      "mean       0.636467      0.494792      0.626262      0.560934      0.534188   \n",
      "std        0.424384      0.380138      0.305664      0.369693      0.325739   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.361111      0.218692      0.240000   \n",
      "50%        1.000000      0.500000      0.666667      0.666667      0.473684   \n",
      "75%        1.000000      1.000000      1.000000      1.000000      0.875000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "                1.5            +1  \n",
      "count  34464.000000  34464.000000  \n",
      "mean       0.506679      0.428737  \n",
      "std        0.372904      0.903442  \n",
      "min        0.000000     -1.000000  \n",
      "25%        0.139535     -1.000000  \n",
      "50%        0.478261      1.000000  \n",
      "75%        1.000000      1.000000  \n",
      "max        1.000000      1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Basic EDA\n",
    "print(\"Initial Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa007b",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "Handle missing values with mean imputation\n",
    "\n",
    "Standardize continuous features using StandardScaler\n",
    "\n",
    "Encode categorical features if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65161d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Values\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Impute only numeric columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_numeric = pd.DataFrame(imputer.fit_transform(df[numeric_cols]), columns=numeric_cols)\n",
    "\n",
    "# Combine back with non-numeric columns (if any)\n",
    "df_imputed = pd.concat([df_numeric, df[non_numeric_cols].reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5987fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature / Target Separation\n",
    "X = df_imputed.iloc[:, :-1]\n",
    "y = df_imputed.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079fa8e",
   "metadata": {},
   "source": [
    "Modeling\n",
    "\n",
    "Train/test split\n",
    "\n",
    "Train Logistic Regression and Random Forest classifiers\n",
    "\n",
    "Evaluate models with accuracy, precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "913c19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc37a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Features\n",
    "\n",
    "# Separate numeric and non-numeric columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Drop non-numeric columns for modeling\n",
    "X_numeric = X[numeric_cols]\n",
    "\n",
    "# Train/Test Split with numeric features only\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758ebf2",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n",
    "Compare model performance using metrics and confusion matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e77c295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, change: 0.024854762\n",
      "Epoch 1, change: 1\n",
      "Epoch 18, change: 0.019685692\n",
      "Epoch 2, change: 0.50121554\n",
      "Epoch 19, change: 0.016009834\n",
      "Epoch 3, change: 0.49786032\n",
      "Epoch 20, change: 0.014264126\n",
      "Epoch 4, change: 0.44503256\n",
      "Epoch 21, change: 0.012716885\n",
      "Epoch 5, change: 0.38937869\n",
      "Epoch 22, change: 0.011525611\n",
      "Epoch 6, change: 0.39384479\n",
      "Epoch 23, change: 0.010477661\n",
      "Epoch 7, change: 0.22897759\n",
      "Epoch 24, change: 0.0094697218\n",
      "Epoch 8, change: 0.22551613\n",
      "Epoch 25, change: 0.0086294332\n",
      "Epoch 9, change: 0.23593694\n",
      "Epoch 26, change: 0.0078399636\n",
      "Epoch 10, change: 0.1812172\n",
      "Epoch 27, change: 0.0071092426\n",
      "Epoch 11, change: 0.10723151\n",
      "Epoch 28, change: 0.0064558732\n",
      "Epoch 12, change: 0.090130161\n",
      "Epoch 29, change: 0.0058807658\n",
      "Epoch 13, change: 0.06878339\n",
      "Epoch 30, change: 0.0053320256\n",
      "Epoch 14, change: 0.050517528\n",
      "Epoch 31, change: 0.0048585338\n",
      "Epoch 15, change: 0.035746777\n",
      "Epoch 32, change: 0.0044035565\n",
      "Epoch 16, change: 0.025277543\n",
      "Epoch 33, change: 0.0040145247\n",
      "Epoch 17, change: 0.018644835\n",
      "Epoch 34, change: 0.003660334\n",
      "Epoch 18, change: 0.016683679\n",
      "Epoch 35, change: 0.0033306947\n",
      "Epoch 19, change: 0.014940668\n",
      "Epoch 36, change: 0.0030262636\n",
      "max_iter reached after 156 secondsEpoch 20, change: 0.013416827\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, change: 0.0027684977\n",
      "Epoch 38, change: 0.0025143655\n",
      "Epoch 39, change: 0.002286579\n",
      "Epoch 40, change: 0.0020807698\n",
      "Epoch 41, change: 0.0019037741\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=20, solver='saga', n_jobs=-1, class_weight='balanced', verbose=1)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_preds = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, change: 0.001798513\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(confusion_matrix(y_test, rf_preds), annot=True, fmt='d')\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(classification_report(y_test, lr_preds))\n",
    "print(\"Random Forest Results:\")\n",
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30430973",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n",
    "Random Forest generally performs better than Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b4969",
   "metadata": {},
   "source": [
    "Feature Importance\n",
    "\n",
    "Use feature importances from Random Forest\n",
    "\n",
    "Visualize top contributing features with horizontal bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552fd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (Random Forest)\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[-10:]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Top 10 Feature Importances - Random Forest\")\n",
    "plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [f\"Feature {i}\" for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529b588",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Random Forest is effective for deduplication tasks\n",
    "\n",
    "Key features influencing deduplication were identified\n",
    "\n",
    "Future work includes trying XGBoost and improving preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
